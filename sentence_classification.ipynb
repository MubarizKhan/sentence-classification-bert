{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence-classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Wfw3oe1qc_6",
        "outputId": "b5fa973f-7c91-47f7-d745-45a666c4bba5"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "  print('Found GPU: {}'.format(device_name))\n",
        "else:\n",
        "  raise SystemError('GPU is not available')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phZp6Q0z6CQz",
        "outputId": "77e4f41a-410b-430a-853e-d93a18417efc"
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print('There are %d GPU(s) available.' %torch.cuda.device_count())\n",
        "  print('we will use GPU:', torch.cuda.device_count() )\n",
        "else:\n",
        "  print('No GPU available, using CPU instead')\n",
        "  device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "we will use GPU: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "id": "odG2kPZM7EP1",
        "outputId": "0bddb5f1-9aec-4cdc-9ab5-763fca610310"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 71.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 66.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 63.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFt9cSnc9aaz",
        "outputId": "9e31fffb-50c7-444d-efe1-a788328af1be"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9672 sha256=1fd73b0ca9460007b698338ee5983f66a36ba259d0944f6df33d7fa9947e0847\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZMxDn3M_DE9",
        "outputId": "f1052525-7e45-4522-982e-5e61d643f7b0"
      },
      "source": [
        "import wget\n",
        "import os\n",
        "\n",
        "print('Downloading dataset...')\n",
        "# The URL for the dataset zip file.\n",
        "url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "# Download the file (if we haven't already)\n",
        "if not os.path.exists('./cola_public_1.1.zip'):\n",
        "    wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading dataset...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zs45JS7_IPM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd8d059-f46a-49dc-929e-ed7322fa2ffb"
      },
      "source": [
        "if not os.path.exists('./cola_public/'):\n",
        "  !unzip cola_public_1.1.zip\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  cola_public_1.1.zip\n",
            "   creating: cola_public/\n",
            "  inflating: cola_public/README      \n",
            "   creating: cola_public/tokenized/\n",
            "  inflating: cola_public/tokenized/in_domain_dev.tsv  \n",
            "  inflating: cola_public/tokenized/in_domain_train.tsv  \n",
            "  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n",
            "   creating: cola_public/raw/\n",
            "  inflating: cola_public/raw/in_domain_dev.tsv  \n",
            "  inflating: cola_public/raw/in_domain_train.tsv  \n",
            "  inflating: cola_public/raw/out_of_domain_dev.tsv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "qYezujhu_XK5",
        "outputId": "ab1a6f1a-d56c-4fc3-f0e5-0ca828bfe0d5"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 8,551\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_source</th>\n",
              "      <th>label</th>\n",
              "      <th>label_notes</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4609</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>John has taken Bill to the library.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5810</th>\n",
              "      <td>c_13</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>That automobile factories abound in Michigan w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It seems that John is likely to win.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8138</th>\n",
              "      <td>ad03</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Which ode did which poet write?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3223</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Caesar put a gushing fountain by his palace.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>bc01</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Mag Wildwood came to introduce the bartender b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3565</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>I am anxious for you to study English grammar ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3147</th>\n",
              "      <td>l-93</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Gloria dozed.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5156</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>It was with a great deal of regret that I veto...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4035</th>\n",
              "      <td>ks08</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>They believe that Charles Darwin's theory of e...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     sentence_source  ...                                           sentence\n",
              "4609            ks08  ...                John has taken Bill to the library.\n",
              "5810            c_13  ...  That automobile factories abound in Michigan w...\n",
              "394             bc01  ...               It seems that John is likely to win.\n",
              "8138            ad03  ...                    Which ode did which poet write?\n",
              "3223            l-93  ...       Caesar put a gushing fountain by his palace.\n",
              "888             bc01  ...  Mag Wildwood came to introduce the bartender b...\n",
              "3565            ks08  ...  I am anxious for you to study English grammar ...\n",
              "3147            l-93  ...                                      Gloria dozed.\n",
              "5156            ks08  ...  It was with a great deal of regret that I veto...\n",
              "4035            ks08  ...  They believe that Charles Darwin's theory of e...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVFcloWwBqtv"
      },
      "source": [
        "sentences = df.sentence.values\n",
        "labels = df.label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA5qRtvU_daH",
        "outputId": "611de867-c542-43d4-b312-494249e0fac9"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "print('Loading Bert Tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Bert Tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sMYRWmsA1b_",
        "outputId": "29c30fdb-c6b3-40b2-8706-23b38871856e"
      },
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o5nONMrISQC",
        "outputId": "b45d3c9f-a7c0-42e7-bb33-67e0e55f6563"
      },
      "source": [
        "text = 'whdat do you do?'\n",
        "print('original-text {:,}.',text)\n",
        "print('tokenized', tokenizer.tokenize(text))\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original-text {:,}. whdat do you do?\n",
            "tokenized ['w', '##hd', '##at', 'do', 'you', 'do', '?']\n",
            "Token IDs:  [1059, 14945, 4017, 2079, 2017, 2079, 1029]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0371A3eGBQHB",
        "outputId": "6183a011-1544-4e69-9cdf-6044258733a8"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "# max_length = 64\n",
        "\n",
        "for sent in sentences:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "      sent,\n",
        "      add_special_tokens = True,\n",
        "      max_length = 64,\n",
        "      # pad_to_max_length = True,\n",
        "      padding='max_length',\n",
        "      # truncation='true',\n",
        "      return_attention_mask = True,\n",
        "      return_tensors = 'pt',\n",
        "  )\n",
        "\n",
        "  input_ids.append(encoded_dict['input_ids'])\n",
        "  attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "\n",
        "print('original:', sentences[0])\n",
        "print('Token IDs', input_ids[0])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original: Our friends won't buy this analysis, let alone the next one we propose.\n",
            "Token IDs tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
            "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ovsFQ47DaxA",
        "outputId": "4f70564b-06ca-4af0-a6e3-e03b681f7e0c"
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size \n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "print('{:>5,} training_samples'.format(train_size))\n",
        "print('{:>5,} validation_samples'.format(val_size)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7,695 training_samples\n",
            "  856 validation_samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2-jZYSNG1kL"
      },
      "source": [
        "# from torch.utils.data import Dataloader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler = RandomSampler(train_dataset),\n",
        "    batch_size = batch_size\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    sampler = SequentialSampler(val_dataset),\n",
        "    batch_size = batch_size\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nYyl0M6H7xz",
        "outputId": "db6c697d-2c3d-40da-e2ac-6ec1a717a0bb"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgkWOUnJJhxf"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8,\n",
        "                  )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PsLVwLIJ_aP"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 4\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg1C-MgGKw1o"
      },
      "source": [
        "import numpy as np\n",
        "def flat_accuracy(preds,labels):\n",
        "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgk7HU95KxEQ"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round(elapsed))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf8g8hEULHOJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-FYdx6nFE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c3e30b-eb5d-4b13-a207-05a48a87ed2a"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "        # function and pass down the arguments. The `forward` function is \n",
        "        # documented here: \n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:12.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:29.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:35.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:00:35\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:11.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:29.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:34.\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.45\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:11.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:29.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:34.\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation Loss: 0.50\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:06.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:11.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:17.\n",
            "  Batch   160  of    241.    Elapsed: 0:00:23.\n",
            "  Batch   200  of    241.    Elapsed: 0:00:29.\n",
            "  Batch   240  of    241.    Elapsed: 0:00:34.\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epcoh took: 0:00:34\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.57\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:02:22 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X9fZf7hKxTn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2513eda2-7d0b-4122-d3f2-265085356da6"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # print('This is step {:>5,}. ', type(step))\n",
        "        # print('This is Batch[0] {:,}.',type(batch))\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "       \n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
        "        # function and pass down the arguments. The `forward` function is \n",
        "        # documented here: \n",
        "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
        "        # The results are returned in a results object, documented here:\n",
        "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
        "        # Specifically, we'll get the loss (because we provided labels) and the\n",
        "        # \"logits\"--the model outputs prior to activation.\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        # print('This is loss {:,}. '.format(loss))\n",
        "        # print('This is loss{:,}. '.format(b_input_mask))\n",
        "        # print('This is b_labels {:,}. '.format(b_labels))\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "\n",
        "        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "        # output values prior to applying an activation function like the \n",
        "        # softmax.\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits,label_ids)\n",
        "        \n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:16.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:32.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:47.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:01.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:16.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:31.\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:01:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.83\n",
            "  Validation Loss: 0.39\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:45.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:00.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:16.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:31.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:01:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.42\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:45.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:00.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:15.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:30.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:01:31\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation Loss: 0.44\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    241.    Elapsed: 0:00:15.\n",
            "  Batch    80  of    241.    Elapsed: 0:00:30.\n",
            "  Batch   120  of    241.    Elapsed: 0:00:45.\n",
            "  Batch   160  of    241.    Elapsed: 0:01:00.\n",
            "  Batch   200  of    241.    Elapsed: 0:01:15.\n",
            "  Batch   240  of    241.    Elapsed: 0:01:30.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:01:30\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.54\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:06:18 (h:mm:ss)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hoLBrfhbMR2Q",
        "outputId": "e5133a6b-4dd6-41ff-d3a8-4cb66f46c7b0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option('precision', 2)\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Valid. Loss</th>\n",
              "      <th>Valid. Accur.</th>\n",
              "      <th>Training Time</th>\n",
              "      <th>Validation Time</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>epoch</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.51</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.83</td>\n",
              "      <td>0:01:31</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.32</td>\n",
              "      <td>0.42</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:01:31</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.85</td>\n",
              "      <td>0:01:31</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.13</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.84</td>\n",
              "      <td>0:01:30</td>\n",
              "      <td>0:00:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\n",
              "epoch                                                                         \n",
              "1               0.51         0.39           0.83       0:01:31         0:00:04\n",
              "2               0.32         0.42           0.84       0:01:31         0:00:04\n",
              "3               0.20         0.44           0.85       0:01:31         0:00:04\n",
              "4               0.13         0.54           0.84       0:01:30         0:00:04"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC1B5miBSzrp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "38be7a87-c1a8-4c25-e263-c5b446e99868"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHj2Jai8TS-H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLzGgFj3TTNk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxZ6xGrTTTct"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aArRAWI1TT48"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75srDM_GTUJV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}